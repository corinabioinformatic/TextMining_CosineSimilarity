{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining step by step\n",
    "\n",
    "This is a step by step notebook to show how the module we created before is working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. - Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import  Iterable, List, Set\n",
    "from typing import Any, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "#from vectors import Vector\n",
    "import math\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import sys\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. - Declaring variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = List[float]\n",
    "\n",
    "# >>>>  Most common dimension\n",
    "\n",
    "# We want to ignore these characters,\n",
    "# so that e.g. \"U.S.\", \"U.S\", \"US_\" and \"US\" are the same word.\n",
    "ignore_char_regex = re.compile(\"[\\W_]\")\n",
    "\n",
    "# Has to start and end with an alphanumeric character\n",
    "is_valid_word = re.compile(\"^[^\\W_].*[^\\W_]$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - Declaring classes &  functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - \"Vectors\" function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_len(v: Vector) -> float:\n",
    "    return math.sqrt(sum([x*x for x in v]))\n",
    "\n",
    "def dot(v1: Vector, v2: Vector) -> float:\n",
    "    assert len(v1) == len(v2)\n",
    "    return sum([x*y for (x,y) in zip(v1, v2)])\n",
    "\n",
    "def vec_add(v1: Vector, v2: Vector) -> Vector:\n",
    "    assert len(v1) == len(v2)\n",
    "    return [x + y for (x,y) in zip(v1, v2)]\n",
    "\n",
    "def vec_sub(v1: Vector, v2: Vector) -> Vector:\n",
    "    assert len(v1) == len(v2)\n",
    "    return [x - y for (x,y) in zip(v1, v2)]\n",
    "\n",
    "def vec_normalize(v: Vector) -> Vector:\n",
    "    l = l2_len(v)\n",
    "    return [x / l for x in v]\n",
    "\n",
    "def cosine_similarity_normalized(v1: Vector, v2: Vector) -> float:\n",
    "    \"\"\"\n",
    "    Returns the cosine of the angle between the two vectors.\n",
    "    Each of the vectors must have length (L2-norm) equal to 1.\n",
    "    Results range from -1 (very different) to 1 (very similar).\n",
    "    \"\"\"\n",
    "    return dot(v1, v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. -\"Word\" Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "\n",
    "    \"\"\"A single word (one line of the input file)\"\"\"\n",
    "\n",
    "    def __init__(self, text: str, vector: Vector, frequency: int) -> None:\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        vector_preview = ', '.join(map(str, self.vector[:2]))\n",
    "        return f\"{self.text} [{vector_preview}, ...]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. - \"Load\" Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the input file (see https://fasttext.cc/docs/en/english-vectors.html)\n",
    "and do some cleanup.\n",
    "\"\"\"\n",
    "\n",
    "#from typing import Iterable, List, Set\n",
    "\n",
    "#from itertools import groupby\n",
    "#from operator import itemgetter\n",
    "#import re\n",
    "#import vectors as v\n",
    "#from word import Word\n",
    "\n",
    "def load_words(file_path: str) -> List[Word]:\n",
    "    \"\"\"Load and cleanup the data.\"\"\"\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    words = load_words_raw(file_path)\n",
    "    print(f\"Loaded {len(words)} words.\")\n",
    "\n",
    "    #num_dimensions = most_common_dimension(words)\n",
    "    words = [w for w in words if len(w.vector) == 300]\n",
    "    #print(f\"Using {num_dimensions}-dimensional vectors, {len(words)} remain.\")\n",
    "\n",
    "    words = remove_stop_words(words)\n",
    "    print(f\"Removed stop words, {len(words)} remain.\")\n",
    "\n",
    "    words = remove_duplicates(words)\n",
    "    print(f\"Removed duplicates, {len(words)} remain.\")\n",
    "\n",
    "    return words\n",
    "\n",
    "def load_words_raw(file_path: str) -> List[Word]:\n",
    "    \"\"\"Load the file as-is, without doing any validation or cleanup.\"\"\"\n",
    "    def parse_line(line: str, frequency: int) -> Word:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        vector = vec_normalize([float(x) for x in tokens[1:]])\n",
    "        return Word(word, vector, frequency)\n",
    "\n",
    "    words = []\n",
    "    # Words are sorted from the most common to the least common ones\n",
    "    frequency = 1\n",
    "    with open(file_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            w = parse_line(line, frequency)\n",
    "            words.append(w)\n",
    "            frequency += 1\n",
    "    return words\n",
    "\n",
    "def iter_len(iter: Iterable[complex]) -> int:\n",
    "    return sum(1 for _ in iter)\n",
    "\n",
    "def most_common_dimension(words: List[Word]) -> int:\n",
    "    \"\"\"\n",
    "    There is a line in the input file which is missing a word\n",
    "    (search -0.0739, -0.135, 0.0584).\n",
    "    \"\"\"\n",
    "    lengths = sorted([len(word.vector) for word in words])\n",
    "    dimensions = [(k, iter_len(v)) for k, v in groupby(lengths)]\n",
    "    print(\"Dimensions:\")\n",
    "    for (dim, num_vectors) in dimensions:\n",
    "        print(f\"{num_vectors} {dim}-dimensional vectors\")\n",
    "    most_common = sorted(dimensions, key=lambda t: t[1], reverse=True)[0]\n",
    "    return most_common[0]\n",
    "\n",
    "\n",
    "def remove_duplicates(words: List[Word]) -> List[Word]:\n",
    "    seen_words: Set[str] = set()\n",
    "    unique_words: List[Word] = []\n",
    "    for w in words:\n",
    "        canonical = ignore_char_regex.sub(\"\", w.text)\n",
    "        if not canonical in seen_words:\n",
    "            seen_words.add(canonical)\n",
    "            # Keep the original ordering\n",
    "            unique_words.append(w)\n",
    "    return unique_words\n",
    "\n",
    "def remove_stop_words(words: List[Word]) -> List[Word]:\n",
    "    return [w for w in words if (\n",
    "        len(w.text) > 1 and is_valid_word.match(w.text))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 \"cosinesimilarity10words\" FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# 1) # Loading required libraries\n",
    "######\n",
    "\n",
    "#from typing import Any, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "#from load import load_words\n",
    "#import math\n",
    "#import vectors as v\n",
    "#from vectors import Vector\n",
    "#from word import Word\n",
    "\n",
    "# Timing info for most_similar (100k words):\n",
    "# Original version: 7.3s\n",
    "# Normalized vectors: 3.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# 2) # Declaring functions\n",
    "######\n",
    "\n",
    "# 2.1\n",
    "def most_similar(base_vector: Vector, words: List[Word]) -> List[Tuple[float, Word]]:\n",
    "    \"\"\"Finds n words with smallest cosine similarity to a given word\"\"\"\n",
    "    words_with_distance = [(cosine_similarity_normalized(base_vector, w.vector), w) for w in words]\n",
    "    # We want cosine similarity to be as large as possible (close to 1)\n",
    "    sorted_by_distance = sorted(words_with_distance, key=lambda t: t[0], reverse=True)\n",
    "    return sorted_by_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "\n",
    "def print_most_similar(words: List[Word], text: str) -> None:\n",
    "    base_word = find_word(text, words)\n",
    "\n",
    "    if not base_word:\n",
    "        print(f\"Uknown word: {text}\")\n",
    "        return\n",
    "    print(f\"\\n\\tWords related to {base_word.text}:\")\n",
    "    sorted_by_distance = [\n",
    "        word.text for (dist, word) in\n",
    "            most_similar(base_word.vector, words)\n",
    "            if word.text.lower() != base_word.text.lower()\n",
    "        ]\n",
    "    print(', '.join(sorted_by_distance[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4\n",
    "\n",
    "def find_word(text: str, words: List[Word]) -> Optional[Word]:\n",
    "    try:\n",
    "       return next(w for w in words if text == w.text)\n",
    "    except StopIteration:\n",
    "       return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5\n",
    "\n",
    "def closest_analogies(\n",
    "    left2: str, left1: str, right2: str, words: List[Word]\n",
    ") -> List[Tuple[float, Word]]:\n",
    "    word_left1 = find_word(left1, words)\n",
    "    word_left2 = find_word(left2, words)\n",
    "    word_right2 = find_word(right2, words)\n",
    "    if (not word_left1) or (not word_left2) or (not word_right2):\n",
    "        return []\n",
    "    vector = vec_add(\n",
    "        vec_sub(word_left1.vector, word_left2.vector),\n",
    "        word_right2.vector)\n",
    "    closest = most_similar(vector, words)[:10]\n",
    "    def is_redundant(word: str) -> bool:\n",
    "        \"\"\"\n",
    "        Sometimes the two left vectors are so close the answer is e.g.\n",
    "        \"shirt-clothing is like phone-phones\". Skip 'phones' and get the next\n",
    "        suggestion, which might be more interesting.\n",
    "        \"\"\"\n",
    "        word_lower = word.lower()\n",
    "        return (\n",
    "            left1.lower() in word_lower or\n",
    "            left2.lower() in word_lower or\n",
    "            right2.lower() in word_lower)\n",
    "    closest_filtered = [(dist, w) for (dist, w) in closest if not is_redundant(w.text)]\n",
    "    return closest_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6\n",
    "\n",
    "def print_analogy(left2: str, left1: str, right2: str, words: List[Word]) -> None:\n",
    "    analogies = closest_analogies(left2, left1, right2, words)\n",
    "    if (len(analogies) == 0):\n",
    "        print(f\"\\t{left2}-{left1} is like {right2}-?\\n\")\n",
    "    else:\n",
    "        (dist, w) = analogies[0]\n",
    "        #alternatives = ', '.join([f\"{w.text} ({dist})\" for (dist, w) in analogies])\n",
    "        print(f\"\\t{left2}-{left1} is like {right2}-{w.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. - Executing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing code for functions previously loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# >>>> remove stop words FUNCTION\n",
    "\n",
    "# Run \"smoke tests\" on import - \n",
    "assert [w.text for w in remove_stop_words([\n",
    "    Word('a', [], 1),\n",
    "    Word('ab', [], 1),\n",
    "    Word('-ab', [], 1),\n",
    "    Word('ab_', [], 1),\n",
    "    Word('a.', [], 1),\n",
    "    Word('.a', [], 1),\n",
    "    Word('ab', [], 1),\n",
    "])] == ['ab', 'ab']\n",
    "assert [w.text for w in remove_duplicates([\n",
    "    Word('a.b', [], 1),\n",
    "    Word('-a-b', [], 1),\n",
    "    Word('ab_+', [], 1),\n",
    "    Word('.abc...', [], 1),\n",
    "])] == ['a.b', '.abc...']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is is the test script from original source to see how accurate are the predictions with some examples.\n",
    "It does not need any input argument. Just run. To introduce an argument for predic, better run in command prompt:\n",
    "\n",
    "**cosinesimilarity10words.py + wordTarget**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/wordvectors/words_test2.vec...\n",
      "Loaded 120221 words.\n",
      "Removed stop words, 118458 remain.\n",
      "Removed duplicates, 116380 remain.\n",
      "--- 29.106871843338013 seconds ---\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# 3) # Load vec file\n",
    "######\n",
    "start_time = time.time()\n",
    "\n",
    "#O# words = load_words('data/words.vec')\n",
    "words = load_words('../data/wordvectors/words_test2.vec') #Running the biggest wordvector provided by fasttext\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting 10 similar words to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWords related to sugar:\n",
      "sugars, sucrose, syrup, glucose, molasses, fructose, sugary, sugarcane, carbohydrates, cocoa\n"
     ]
    }
   ],
   "source": [
    "# Print  the 10 most similar words to SUGAR!\n",
    "start_time = time.time()\n",
    "print_most_similar(words, \"sugar\")# str(sys.argv[1]))\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWords related to diabetes:\n",
      "diabetic, diabetics, hypertension, mellitus, obesity, insulin, hypoglycemia, asthma, Diabetic, disease\n",
      "\n",
      "--- 5.120102643966675 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Print  the 10 most similar words to DIABETES!\n",
    "start_time = time.time()\n",
    "print_most_similar(words, \"diabetes\")# str(sys.argv[1]))\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
